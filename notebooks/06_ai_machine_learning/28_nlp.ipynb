{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df17522",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you will be able to:\n",
    "- Process and analyze text data\n",
    "- Build sentiment analysis systems\n",
    "- Use pre-trained language models\n",
    "- Create text classification applications\n",
    "\n",
    "## Core Concepts\n",
    "- **Tokenization**: Breaking text into words or pieces\n",
    "- **Embeddings**: Converting words to numbers that capture meaning\n",
    "- **Transformer**: Modern AI architecture that understands context\n",
    "- **Sentiment Analysis**: Determining if text is positive, negative, or neutral\n",
    "- **Classification**: Automatically categorizing text (spam, topics, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f6a02",
   "metadata": {},
   "source": [
    "## 1. Text Processing Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6106f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üìù TEXT PROCESSING FUNDAMENTALS\")\n",
    "\n",
    "# Sample texts for analysis\n",
    "texts = [\n",
    "    \"I love this product! It's amazing and works perfectly.\",\n",
    "    \"This is terrible. Worst purchase ever. Totally disappointed.\",\n",
    "    \"The item is okay. Nothing special but does the job.\",\n",
    "    \"Fantastic! Exceeded my expectations. Highly recommend!\",\n",
    "    \"Not bad, could be better. Average quality for the price.\",\n",
    "    \"Absolutely horrible. Waste of money. Do not buy!\",\n",
    "    \"Great value! Works as described. Very satisfied.\",\n",
    "    \"Meh. It's fine I guess. Neither good nor bad.\"\n",
    "]\n",
    "\n",
    "labels = [1, 0, 0, 1, 0, 0, 1, 0]  # 1=positive, 0=negative/neutral\n",
    "\n",
    "print(\"Sample reviews:\")\n",
    "for i, (text, label) in enumerate(zip(texts, labels)):\n",
    "    sentiment = \"Positive\" if label == 1 else \"Negative/Neutral\"\n",
    "    print(f\"{i+1}. {text}\")\n",
    "    print(f\"   ‚Üí {sentiment}\\n\")\n",
    "\n",
    "# Basic text cleaning\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "print(\"üßπ TEXT CLEANING:\")\n",
    "sample_text = \"I LOVE this!!! It's amazing... Really great product.\"\n",
    "cleaned = clean_text(sample_text)\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Cleaned:  {cleaned}\")\n",
    "\n",
    "# Tokenization (breaking into words)\n",
    "print(f\"\\nüî§ TOKENIZATION:\")\n",
    "words = cleaned.split()\n",
    "print(f\"Words: {words}\")\n",
    "print(f\"Number of words: {len(words)}\")\n",
    "\n",
    "# Word frequency analysis\n",
    "all_text = ' '.join([clean_text(t) for t in texts])\n",
    "word_freq = {}\n",
    "for word in all_text.split():\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "# Show top words\n",
    "top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(f\"\\nMost common words:\")\n",
    "for word, count in top_words:\n",
    "    print(f\"'{word}': {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df1f5c0",
   "metadata": {},
   "source": [
    "## 2. Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sentiment analysis model\n",
    "print(\"üéØ BUILDING SENTIMENT ANALYSIS MODEL\")\n",
    "\n",
    "# Create larger dataset for training\n",
    "def generate_reviews():\n",
    "    positive_reviews = [\n",
    "        \"amazing product love it\", \"fantastic quality great value\", \"excellent service recommend\",\n",
    "        \"wonderful experience happy customer\", \"outstanding performance very satisfied\",\n",
    "        \"brilliant design works perfectly\", \"incredible quality exceeded expectations\",\n",
    "        \"superb product definitely recommend\", \"awesome features great price\",\n",
    "        \"perfect solution exactly needed\"\n",
    "    ]\n",
    "    \n",
    "    negative_reviews = [\n",
    "        \"terrible quality waste money\", \"horrible experience never again\", \"awful product disappointed\",\n",
    "        \"worst purchase ever regret\", \"cheap quality broke immediately\", \"useless product poor design\",\n",
    "        \"disappointing results not worth\", \"bad service poor quality\", \"failed expectations terrible\",\n",
    "        \"completely useless avoid buying\"\n",
    "    ]\n",
    "    \n",
    "    # Combine and create labels\n",
    "    all_reviews = positive_reviews + negative_reviews\n",
    "    all_labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)\n",
    "    \n",
    "    return all_reviews, all_labels\n",
    "\n",
    "reviews, sentiment_labels = generate_reviews()\n",
    "print(f\"Dataset: {len(reviews)} reviews ({sum(sentiment_labels)} positive, {len(sentiment_labels)-sum(sentiment_labels)} negative)\")\n",
    "\n",
    "# Convert text to numbers using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "y = np.array(sentiment_labels)\n",
    "\n",
    "print(f\"Text converted to {X.shape[1]} features\")\n",
    "\n",
    "# Split and train model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(f\"Model accuracy: {accuracy:.1%}\")\n",
    "\n",
    "# Test on new reviews\n",
    "test_reviews = [\n",
    "    \"This product is absolutely fantastic!\",\n",
    "    \"Completely disappointed with this purchase\",\n",
    "    \"Pretty good, works as expected\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüîç TESTING ON NEW REVIEWS:\")\n",
    "for review in test_reviews:\n",
    "    # Clean and vectorize\n",
    "    cleaned_review = clean_text(review)\n",
    "    review_vector = vectorizer.transform([cleaned_review])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(review_vector)[0]\n",
    "    probability = model.predict_proba(review_vector)[0]\n",
    "    \n",
    "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    confidence = max(probability)\n",
    "    \n",
    "    print(f\"'{review}'\")\n",
    "    print(f\"‚Üí {sentiment} (confidence: {confidence:.2f})\")\n",
    "\n",
    "# Show important words\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Top positive words\n",
    "pos_indices = coefficients.argsort()[-5:][::-1]\n",
    "print(f\"\\nTop positive words:\")\n",
    "for idx in pos_indices:\n",
    "    print(f\"'{feature_names[idx]}': {coefficients[idx]:.3f}\")\n",
    "\n",
    "# Top negative words  \n",
    "neg_indices = coefficients.argsort()[:5]\n",
    "print(f\"\\nTop negative words:\")\n",
    "for idx in neg_indices:\n",
    "    print(f\"'{feature_names[idx]}': {coefficients[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a4ae70",
   "metadata": {},
   "source": [
    "## 3. Advanced Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text classification and topic analysis\n",
    "print(\"üìä ADVANCED TEXT ANALYSIS\")\n",
    "\n",
    "# Topic classification example\n",
    "topics_data = {\n",
    "    'text': [\n",
    "        \"The stock market rose today with tech companies leading gains\",\n",
    "        \"New smartphone features include better camera and longer battery\", \n",
    "        \"Scientists discover new treatment for cancer using AI\",\n",
    "        \"Football team wins championship after dramatic final game\",\n",
    "        \"Recipe for delicious chocolate cake with simple ingredients\",\n",
    "        \"Investment strategies for retirement planning and wealth building\",\n",
    "        \"Latest iPhone review compares features with Android phones\",\n",
    "        \"Medical research shows benefits of exercise for heart health\",\n",
    "        \"Basketball player breaks scoring record in playoff game\",\n",
    "        \"Cooking tips for perfect pasta and Italian dishes\"\n",
    "    ],\n",
    "    'category': ['Finance', 'Technology', 'Health', 'Sports', 'Food', \n",
    "                'Finance', 'Technology', 'Health', 'Sports', 'Food']\n",
    "}\n",
    "\n",
    "topics_df = pd.DataFrame(topics_data)\n",
    "print(\"Topic classification dataset:\")\n",
    "print(topics_df)\n",
    "\n",
    "# Convert categories to numbers\n",
    "category_map = {'Finance': 0, 'Technology': 1, 'Health': 2, 'Sports': 3, 'Food': 4}\n",
    "topics_df['label'] = topics_df['category'].map(category_map)\n",
    "\n",
    "# Build multi-class classifier\n",
    "topic_vectorizer = TfidfVectorizer(max_features=50, stop_words='english')\n",
    "X_topics = topic_vectorizer.fit_transform(topics_df['text'])\n",
    "y_topics = topics_df['label']\n",
    "\n",
    "topic_model = LogisticRegression(multi_class='ovr')\n",
    "topic_model.fit(X_topics, y_topics)\n",
    "\n",
    "# Test topic classification\n",
    "test_texts = [\n",
    "    \"Apple stock price increases after earnings report\",\n",
    "    \"New Android phone has amazing camera quality\", \n",
    "    \"Doctor recommends healthy diet for diabetes\",\n",
    "    \"Tennis player wins tournament in straight sets\",\n",
    "    \"Best pizza recipe with homemade dough\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüéØ TOPIC CLASSIFICATION:\")\n",
    "for text in test_texts:\n",
    "    text_vector = topic_vectorizer.transform([text])\n",
    "    prediction = topic_model.predict(text_vector)[0]\n",
    "    probabilities = topic_model.predict_proba(text_vector)[0]\n",
    "    \n",
    "    categories = list(category_map.keys())\n",
    "    predicted_category = categories[prediction]\n",
    "    confidence = probabilities[prediction]\n",
    "    \n",
    "    print(f\"'{text[:50]}...'\")\n",
    "    print(f\"‚Üí {predicted_category} (confidence: {confidence:.2f})\")\n",
    "\n",
    "# Practice Exercises\n",
    "print(f\"\\nüìö PRACTICE EXERCISES:\")\n",
    "\n",
    "# Exercise 1: Email spam detection\n",
    "print(f\"\\nExercise 1: Email Spam Detection\")\n",
    "emails = [\n",
    "    \"Meeting at 3pm in conference room A\",\n",
    "    \"Congratulations! You won $1000000! Click here now!\",\n",
    "    \"Project deadline moved to next Friday\",\n",
    "    \"URGENT: Claim your prize immediately or lose forever!\",\n",
    "    \"Can you review the quarterly report before Monday?\",\n",
    "    \"FREE MONEY!!! No strings attached! Act now!\"\n",
    "]\n",
    "\n",
    "spam_labels = [0, 1, 0, 1, 0, 1]  # 0=not spam, 1=spam\n",
    "print(\"Build a spam detector using these emails\")\n",
    "\n",
    "# Exercise 2: Product review analysis  \n",
    "print(f\"\\nExercise 2: Product Review Analysis\")\n",
    "product_reviews = [\n",
    "    \"Great phone, battery lasts all day\",\n",
    "    \"Screen is too small, hard to read\",\n",
    "    \"Fast delivery, excellent packaging\", \n",
    "    \"Expensive but worth the quality\",\n",
    "    \"Poor customer service experience\"\n",
    "]\n",
    "print(\"Analyze these reviews for different aspects (battery, screen, service, etc.)\")\n",
    "\n",
    "# Exercise 3: Keyword extraction\n",
    "print(f\"\\nExercise 3: Keyword Extraction\")\n",
    "document = \"Machine learning is transforming healthcare by enabling AI systems to analyze medical data and predict patient outcomes\"\n",
    "words = document.lower().split()\n",
    "word_counts = {}\n",
    "for word in words:\n",
    "    if len(word) > 3:  # Ignore short words\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "print(\"Extract important keywords from documents\")\n",
    "top_keywords = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "print(f\"Top keywords: {[word for word, count in top_keywords]}\")\n",
    "\n",
    "# Exercise 4: Text similarity\n",
    "print(f\"\\nExercise 4: Text Similarity\") \n",
    "def simple_similarity(text1, text2):\n",
    "    words1 = set(text1.lower().split())\n",
    "    words2 = set(text2.lower().split())\n",
    "    \n",
    "    intersection = words1.intersection(words2)\n",
    "    union = words1.union(words2)\n",
    "    \n",
    "    return len(intersection) / len(union) if union else 0\n",
    "\n",
    "text_a = \"I love machine learning and artificial intelligence\"\n",
    "text_b = \"Machine learning and AI are fascinating topics\"\n",
    "similarity = simple_similarity(text_a, text_b)\n",
    "\n",
    "print(f\"Text A: {text_a}\")\n",
    "print(f\"Text B: {text_b}\")\n",
    "print(f\"Similarity: {similarity:.2f}\")\n",
    "\n",
    "# Key insights\n",
    "print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "print(f\"‚úÖ Clean text before analysis (lowercase, remove punctuation)\")\n",
    "print(f\"‚úÖ TF-IDF converts text to numbers while preserving meaning\")\n",
    "print(f\"‚úÖ Logistic regression works well for text classification\")\n",
    "print(f\"‚úÖ Feature selection is important for large vocabularies\")\n",
    "print(f\"‚úÖ Always test on new, unseen text data\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(f\"1. Try pre-trained models (BERT, GPT)\")\n",
    "print(f\"2. Use word embeddings (Word2Vec, GloVe)\")\n",
    "print(f\"3. Experiment with neural networks for text\")\n",
    "print(f\"4. Learn about transformers and attention\")\n",
    "print(f\"5. Build chatbots and language models\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
